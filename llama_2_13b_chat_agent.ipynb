{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JUNW2021/dimspace-pub/blob/main/llama_2_13b_chat_agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_fRq0BSGMBk"
      },
      "outputs": [],
      "source": [
        "!pip install -qU transformers accelerate einops langchain xformers bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHQwEeW9Zps2"
      },
      "source": [
        "## Initializing the Hugging Face Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mElf068NXout"
      },
      "source": [
        "The first thing we need to do is initialize a `text-generation` pipeline with Hugging Face transformers. The Pipeline requires three things that we must initialize first, those are:\n",
        "\n",
        "* A LLM, in this case it will be `meta-llama/Llama-2-70b-chat-hf`.\n",
        "\n",
        "* The respective tokenizer for the model.\n",
        "\n",
        "* A stopping criteria object.\n",
        "\n",
        "We'll explain these as we get to them, let's begin with our model.\n",
        "\n",
        "We initialize the model and move it to our CUDA-enabled GPU. Using Colab this can take 5-10 minutes to download and initialize the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikzdi_uMI7B-"
      },
      "outputs": [],
      "source": [
        "from torch import cuda, bfloat16\n",
        "import transformers\n",
        "\n",
        "model_id = 'meta-llama/Llama-2-13b-chat-hf'\n",
        "\n",
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
        "\n",
        "# set quantization configuration to load large model with less GPU memory\n",
        "# this requires the `bitsandbytes` library\n",
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=bfloat16\n",
        ")\n",
        "\n",
        "# begin initializing HF items, need auth token for these\n",
        "hf_auth = ''\n",
        "model_config = transformers.AutoConfig.from_pretrained(\n",
        "    model_id,\n",
        "    use_auth_token=hf_auth\n",
        ")\n",
        "\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    config=model_config,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto',\n",
        "    use_auth_token=hf_auth\n",
        ")\n",
        "model.eval()\n",
        "print(f\"Model loaded on {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzX9LqWSX9ot"
      },
      "source": [
        "The pipeline requires a tokenizer which handles the translation of human readable plaintext to LLM readable token IDs. The Llama 2 70B models were trained using the Llama 2 70B tokenizer, which we initialize like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0iPv1GDGxgT"
      },
      "outputs": [],
      "source": [
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "    model_id,\n",
        "    use_auth_token=hf_auth\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XL7G9Sr3uxdz"
      },
      "source": [
        "Finally we need to define the _stopping criteria_ of the model. The stopping criteria allows us to specify *when* the model should stop generating text. If we don't provide a stopping criteria the model just goes on a bit of a tangent after answering the initial question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_B84bA1j9pH"
      },
      "outputs": [],
      "source": [
        "{\n",
        "    \"action\": \"Calculator\",\n",
        "    \"action_input\": \"2+2\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0iTGu6YFRWs"
      },
      "outputs": [],
      "source": [
        "stop_list = ['\\nHuman:', '\\n```\\n']\n",
        "\n",
        "stop_token_ids = [tokenizer(x)['input_ids'] for x in stop_list]\n",
        "stop_token_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IoQifZvEFD_"
      },
      "source": [
        "We need to convert these into `LongTensor` objects:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIzaQ24TEJES"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]\n",
        "stop_token_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1znn7p1ESte"
      },
      "source": [
        "We can do a quick spot check that no `<unk>` token IDs (`0`) appear in the `stop_token_ids` — there are none so we can move on to building the stopping criteria object that will check whether the stopping criteria has been satisfied — meaning whether any of these token ID combinations have been generated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXPcO0FED5Jo"
      },
      "outputs": [],
      "source": [
        "from transformers import StoppingCriteria, StoppingCriteriaList\n",
        "\n",
        "# define custom stopping criteria object\n",
        "class StopOnTokens(StoppingCriteria):\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
        "        for stop_ids in stop_token_ids:\n",
        "            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "stopping_criteria = StoppingCriteriaList([StopOnTokens()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNysQFtPoaj7"
      },
      "source": [
        "Now we're ready to initialize the HF pipeline. There are a few additional parameters that we must define here. Comments explaining these have been included in the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAYXi8ayKusU"
      },
      "outputs": [],
      "source": [
        "generate_text = transformers.pipeline(\n",
        "    model=model, tokenizer=tokenizer,\n",
        "    return_full_text=True,  # langchain expects the full text\n",
        "    task='text-generation',\n",
        "    # we pass model parameters here too\n",
        "    #stopping_criteria=stopping_criteria,  # without this model rambles during chat\n",
        "    temperature=0.0,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
        "    max_new_tokens=512,  # mex number of tokens to generate in the output\n",
        "    repetition_penalty=1.1  # without this output begins repeating\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DG1WNTnJF1o"
      },
      "source": [
        "Confirm this is working:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhFgmMr0JHUF"
      },
      "outputs": [],
      "source": [
        "res = generate_text(\"Explain to me the difference between nuclear fission and fusion.\")\n",
        "print(res[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0N3W3cj3Re1K"
      },
      "source": [
        "Now to implement this in LangChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8RxQYwHRg0N"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=generate_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aiW0_FoQWG6J"
      },
      "outputs": [],
      "source": [
        "llm(prompt=\"Explain to me the difference between nuclear fission and fusion.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tv0KxJLvsIa"
      },
      "source": [
        "We still get the same output as we're not really doing anything differently here, but we have now added **Llama 2 70B Chat** to the LangChain library. Using this we can now begin using LangChain's advanced agent tooling, chains, etc, with **Llama 2**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjTR6XL54fc8"
      },
      "source": [
        "## Initializing an Conversational Agent\n",
        "\n",
        "Getting a conversational agent to work with open source models is incredibly hard. However, with Llama 2 70B it is now possible. Let's see how we can get it running!\n",
        "\n",
        "We first need to initialize the agent. Conversational agents require several things such as conversational `memory`, access to `tools`, and an `llm` (which we have already initialized)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmfX0_usXTx3"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain.agents import load_tools\n",
        "\n",
        "memory = ConversationBufferWindowMemory(\n",
        "    memory_key=\"chat_history\", k=5, return_messages=True, output_key=\"output\"\n",
        ")\n",
        "tools = load_tools([\"llm-math\"], llm=llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0jhbRMw48ep"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import AgentOutputParser\n",
        "from langchain.agents.conversational_chat.prompt import FORMAT_INSTRUCTIONS\n",
        "from langchain.output_parsers.json import parse_json_markdown\n",
        "from langchain.schema import AgentAction, AgentFinish\n",
        "\n",
        "class OutputParser(AgentOutputParser):\n",
        "    def get_format_instructions(self) -> str:\n",
        "        return FORMAT_INSTRUCTIONS\n",
        "\n",
        "    def parse(self, text: str) -> AgentAction | AgentFinish:\n",
        "        try:\n",
        "            # this will work IF the text is a valid JSON with action and action_input\n",
        "            response = parse_json_markdown(text)\n",
        "            action, action_input = response[\"action\"], response[\"action_input\"]\n",
        "            if action == \"Final Answer\":\n",
        "                # this means the agent is finished so we call AgentFinish\n",
        "                return AgentFinish({\"output\": action_input}, text)\n",
        "            else:\n",
        "                # otherwise the agent wants to use an action, so we call AgentAction\n",
        "                return AgentAction(action, action_input, text)\n",
        "        except Exception:\n",
        "            # sometimes the agent will return a string that is not a valid JSON\n",
        "            # often this happens when the agent is finished\n",
        "            # so we just return the text as the output\n",
        "            return AgentFinish({\"output\": text}, text)\n",
        "\n",
        "    @property\n",
        "    def _type(self) -> str:\n",
        "        return \"conversational_chat\"\n",
        "\n",
        "# initialize output parser for agent\n",
        "parser = OutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPwa9SWP4-dR"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import initialize_agent\n",
        "\n",
        "# initialize agent\n",
        "agent = initialize_agent(\n",
        "    agent=\"chat-conversational-react-description\",\n",
        "    tools=tools,\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    early_stopping_method=\"generate\",\n",
        "    memory=memory,\n",
        "    #agent_kwargs={\"output_parser\": parser}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Re7e6Ir361xV"
      },
      "outputs": [],
      "source": [
        "agent.agent.llm_chain.prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjFtnFTcOH9_"
      },
      "source": [
        "We need to add special tokens used to signify the beginning and end of instructions, and the beginning and end of system messages. These are described in the Llama-2 model cards on Hugging Face."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mt8Ah5cRMBqN"
      },
      "outputs": [],
      "source": [
        "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
        "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzrpXdVA8K9r"
      },
      "outputs": [],
      "source": [
        "sys_msg = B_SYS + \"\"\"Assistant is a expert JSON builder designed to assist with a wide range of tasks.\n",
        "\n",
        "Assistant is able to respond to the User and use tools using JSON strings that contain \"action\" and \"action_input\" parameters.\n",
        "\n",
        "All of Assistant's communication is performed using this JSON format.\n",
        "\n",
        "Assistant can also use tools by responding to the user with tool use instructions in the same \"action\" and \"action_input\" JSON format. Tools available to Assistant are:\n",
        "\n",
        "- \"Calculator\": Useful for when you need to answer questions about math.\n",
        "  - To use the calculator tool, Assistant should write like so:\n",
        "    ```json\n",
        "    {{\"action\": \"Calculator\",\n",
        "      \"action_input\": \"sqrt(4)\"}}\n",
        "    ```\n",
        "\n",
        "Here are some previous conversations between the Assistant and User:\n",
        "\n",
        "User: Hey how are you today?\n",
        "Assistant: ```json\n",
        "{{\"action\": \"Final Answer\",\n",
        " \"action_input\": \"I'm good thanks, how are you?\"}}\n",
        "```\n",
        "User: I'm great, what is the square root of 4?\n",
        "Assistant: ```json\n",
        "{{\"action\": \"Calculator\",\n",
        " \"action_input\": \"sqrt(4)\"}}\n",
        "```\n",
        "User: 2.0\n",
        "Assistant: ```json\n",
        "{{\"action\": \"Final Answer\",\n",
        " \"action_input\": \"It looks like the answer is 2!\"}}\n",
        "```\n",
        "User: Thanks could you tell me what 4 to the power of 2 is?\n",
        "Assistant: ```json\n",
        "{{\"action\": \"Calculator\",\n",
        " \"action_input\": \"4**2\"}}\n",
        "```\n",
        "User: 16.0\n",
        "Assistant: ```json\n",
        "{{\"action\": \"Final Answer\",\n",
        " \"action_input\": \"It looks like the answer is 16!\"}}\n",
        "```\n",
        "\n",
        "Here is the latest conversation between Assistant and User.\"\"\" + E_SYS\n",
        "new_prompt = agent.agent.create_prompt(\n",
        "    system_message=sys_msg,\n",
        "    tools=tools\n",
        ")\n",
        "agent.agent.llm_chain.prompt = new_prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FT59fuKJVEvB"
      },
      "source": [
        "In the Llama 2 paper they mentioned that it was difficult to keep Llama 2 chat models following instructions over multiple interactions. One way they found that improves this is by inserting a reminder of the instructions to each user query. We do that here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9HSoPJknbmI"
      },
      "outputs": [],
      "source": [
        "instruction = B_INST + \" Respond to the following in JSON with 'action' and 'action_input' values \" + E_INST\n",
        "human_msg = instruction + \"\\nUser: {input}\"\n",
        "\n",
        "agent.agent.llm_chain.prompt.messages[2].prompt.template = human_msg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEgrOtXHEnPy"
      },
      "outputs": [],
      "source": [
        "agent.agent.llm_chain.prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LDoIUbyVbCG"
      },
      "source": [
        "Now we can begin asking questions..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjnM44q76itO"
      },
      "outputs": [],
      "source": [
        "agent(\"hey how are you today?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adBWZl1v-iwA"
      },
      "outputs": [],
      "source": [
        "agent(\"what is 4 to the power of 2.1?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3qWzs1M-sqp"
      },
      "outputs": [],
      "source": [
        "agent(\"can you multiply that by 3?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkO9FR7VUrZS"
      },
      "source": [
        "With that we have our **open source** conversational agent running on Colab with ~38GB of RAM.\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "private_outputs": true,
      "cell_execution_strategy": "setup",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}